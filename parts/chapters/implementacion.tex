% !TEX root = ../../proyect.tex

\chapter{Implementación}\label{implementacion}
\section{Tecnologías}\label{sec:tecnologias}

A continuación se definirán las tecnologías usadas para llevar a cabo el Producto Mínimo Viable.

\subsubsection{\textit{Web Scrapping}}
El aspecto fundamental del producto es la recolección de noticias de periódicos. Esta, como se ha ido diciendo a lo largo del documento, se realiza a través de RSS. Sin esta característica, no tendría utilidad la aplicación. Sin embargo, esto solo se queda corto. Normalmente, cada medio solo publica un titular y una pequeña descripción vía RSS. Esto les sirve de gancho al usuario para que pase del lector de sindicación de contenido a la web de su medio.

Como se ha dicho, otro pilar fundamental para el buen funcionamiento del producto es el análisis de noticias. Por tanto, si no se posee más que un pequeño texto acompañado de cada noticia, poco podrá aportar este. Es por ello imprescindible conseguir el texto original del artículo para poder procesarlo correctamente. Dado que los medios no poseen esta característica, hemos de realizarla a través del \textit{Web Scrapping}.

El \textit{Web Scrapping} es una técnica que permite extraer información de cualquier sitio en Internet de forma automática. La información en la web se encuentra en diferentes formatos dependiendo de su misión para con el receptor. Resumidamente se encuentran dos tipos de datos: estructurados, como pueden ser ficheros XML o APIs, y no estructurados, como los ficheros HTML. Estos últimos abundan ya que, en un principio, van destinados al usuario final directamente, con la idea de ser leídos o vistos sin más.

\figura{0.75}{img/desarrollo/web_scrapping}{Muestra del \textit{Web Scrapping}}{fig:web_scrapping}{}

Por lo dicho, esta técnica posee diferentes implementaciones, dependiendo del nivel de automatización que desee. Un primer nivel sería el de extraer el fichero HTML y trabajar con él, utilizando, por ejemplo, expresiones regulares; otro sería, utilizar parsers o minería de datos, para extraer el contenido principal; y otro, el de analizar una página en concreto, para saber cómo se estructura el contenido realmente interesante.

Como última consideración, se ha informar que antes de trabajar con la información extraída del \textit{Web Scrapping}, hay que saber que esta tiene implicaciones legales. Estas, en la mayoría de los casos, se encuentran en un vacío legal. En la práctica, si los datos se usan para un uso personal, no hay problema añadido. La cosa cambia si es para un uso comercial. De hecho, actualmente, hay varios juicios por un uso fraudulento de esta técnica en Australia o Estados Unidos, como se puede ver en el artículo publicado por \citeA{web_scrapping}.

\subsubsection{Recuperación de información}
Una vez extraída la información de los medios, es imprescindible tratarla correctamente. No basta el guardarla en una base de datos sin más, sino que es necesario usar medios de albergar la información especializado para ello. Si se usa, por ello, ténicas de Búsqueda y Recuperación de Información (ISR, por sus siglar en inglés, en adelante) podemos tratar cada noticia como un documento y así extraer metadatos de cada uno.

\figura{0.75}{img/desarrollo/information_retrieval}{Muestra de Recuperación de Información}{fig:information_retrieval}{}

Esta técnica, por tanto, permite recuperar documentos en base a búsqueda de sus metadatos guardados en un índice. Esta información no suele estar estructurada y su misión es la de buscar información relevante en base a diferentes filtros. Hay diferentes técnicas, dependiendo del grado de exactitud que se requiera teniendo en cuenta la consulta. Algunas de las más importantes son las siguientes.

\begin{itemize}
    \item \textbf{Sistemas de recuperación de lógica difusa}: las búsquedas sobre la información son de tipo fuzzy, es decir, frases normales, que el sistema transforma en frases lógicas. Así, la información que se consegue, se basa en proposiciones lógicas, teniendo en cuenta si los términos buscados aparecen o no.
    \item \textbf{Técnicas de ponderación de términos}: dado que hay términos de búsqueda que tienen más relevancia que otros, es lógico que se pondere dichos términos. Así, los documentos más pertinentes, serán los que contengan todos los términos y que se repita más el que más valor tenga.
    \item \textbf{Técnicas de clustering}: es un modelo probabilístico sobre la frecuencia de los términos de búsqueda. Se clasifican los documentos por valores que, finalmente, representan los pesos.
    \item \textbf{Técnicas de retroalimentación por relevancia}: una vez hecha una búsqueda utilizando una de las técnicas anteriores, se vuelve a buscar pero con términos relacionados con los primeros documentos resultantes.
    \item \textbf{Técnicas de stemming}: usa la técnica de Stemming para la búsqueda, es decir, trunca los términos de búsqueda quitante posibles prefijos o sufijos. En fin, evita ambigüedades léxicas.
\end{itemize}

Como se puede observar, estas técnicas se pueden combinar entre sí dentro de una implementación concreta, aunque siempre el funcionamiento es el presentado en la imagen \ref{fig:information_retrieval}: se procesan los documentos, se indexan y se busca sobre el índice.

Como se ha dicho al inicio del apartado, interesa que este índice creado trate correctamente la información guardardada. Esta es, por tanto, la mayor ventaja de usar un ISR: se pueden elegir técnicas y tratamientos concretos por cada campo, así como por cada índice de documentos. En el ejemplo de guardar noticias se puede decir que extraiga las palabras claves, que de más importancia al cuerpo de la noticia, que obvie mayúsculas y tildes en el índice y para el análisis, que no tenga en cuenta artículos y preposiciones.

Esto dará una versatilidad a esta \textit{base de datos no relacional} mucho mayor, ya que estará especializada en el dominio del problema concreto. Mientras tanto, en una base de datos clásica, la información se guarda siempre de manera homogénea y genérica para cualquier dato que se quiera albergar.

\subsubsection{Sistemas de recomendación}
Una vez obtenidas las noticias y analizadas y albergadas en el sistema habrá que dar uso a los metadatos albergados. Esta, como se ha dicho a la hora de explicar el producto, será el valor diferencial a cualquier lector de noticias: la capacidad de extraer el perfil del usuario y recomendar noticias en base a sus intereses. Esto, por tanto, se consigue con un sistema de recomendación especializado.

\figura{0.5}{img/desarrollo/recommendation_system}{Muestra de sistemas de recomendación}{fig:recommendation_system}{}

Es la técnica de filtrado de información dependiendo del interés del usuario en los mismos. En definitiva, se encarga de comparar el perfil del usuario con los ítems en concreto para, finalmente, recomendarle uno que no haya visto previamente. Para realizar esta técnica, tenemos la capacidad de elección entre diversas implementaciones. Como se puede apreciar, es importante analizar tanto a usuarios como ítems a recomendar.

Analizar el perfil del usuario significa, en definitiva, extraer características de su interactuación con los ítems. El método de extracción de sus características puede ser implícito o explícito. Dentro del primero encontramos aquellos sistemas a los que el sistema solicita al usuario que valore un determinado producto, o, directamente, seleccione sus intereses. Dentro del segundo, a aquellos en los que, en base a visitas a un ítem, al tipo de interacción con el mismo, al tiempo que está viéndolo, u otras técnicas, saca información del usuario.

Ambos tienen aspectos positivos y negativos. En el primero, el usuario dice al sistema expresamente qué le gusta o disgusta, mientras el segundo, la aplicación ha de suponerlo. Esto último puede llevar a numerosos fallos de interpretación frente a la certeza que proporciona el primero. Por otro lado, el método implícito extrae dicha información sin necesidad de la intervención del usuario, cosa que el primero no puede. Esto puede hacer que en el método explícito el usuario nunca complete sus preferencias, por el poco uso de la aplicación.

Como se podrá intuir, la mejor solución depende de la naturaleza de la aplicación y el enfoque que ponga esta en el usuario objetivo. No obstante, una buena solución podrá ser un aproximación a ambas, de forma híbrida. Con esto, se poseerá las ventajas de ambas de manera sencilla.

\figura{0.75}{img/desarrollo/user_profile}{Muestra de extracción del perfil del usuario}{fig:user_profile}{}

Una vez obtenido el perfil del usuario, hemos de poder recomendarle ítems. En este apartado encontramos dos enfoques mayoritarios (y un tercera, de la unión de ambos) como se puede apreciar en la imagen \ref{fig:recommendation_system}.

La primera implementación son las recomendaciones basadas en contenido. Esta técnica consiste en analizar los ítems que tenemos en nuestros sistemas en base a una o varias características. Así, sabiendo los ítems con los que el usuario ha interactuado, podemos recomendarle otros que posean características similares. El principal problema de este sistema de recomendación es la falta de serendipia, es decir, al usuario siempre se le muestra el mismo tipo de productos. Esto se debe a es la información que se tiene y sobre eso se recomienda. Esto conllevará un hartazgo de las recomendaciones, resultando finalmente inútiles.

La segunda implementación es el filtrado colaborativo. Esta consiste en mostrar productos que han resultado atractivos para otros usuarios con gustos similares y que todavía no ha visto. Para esto, se han de calcular similitud entre los usuarios, y no sobre los ítems, como vimos en las recomendaciones en base al contenido. El principal problema del este es que funcionará bien si se poseen diferentes y numerosos tipos de usuario y si se tiene del usuario en cuestión de bastante información para tener un perfil complejo que comparar con otros usuarios. Esto es un problema, para aplicaciones que se empiezan desde cero sin usuarios.

Aquí, igual que anteriormente con el perfil del usuario, es posible combinar ambas técnicas haciendo un filtrado híbrido. Esto hará que el se contrarresten los defectos de ambas, centrándose en sus virtudes. Con respecto a la falta de serendipia, el filtro colaborativo lo contrarresta haciendo similitud entre usuarios, por que el usuario puede verse sorprendido al ver que se le recomienda algo que puede ser nuevo para él. Con respecto a un gran número de usuarios necesarios inicialmente, las recomendaciones basadas en contenido lo contrarrestan por la similitud de ítems, por lo que, por ejemplo, lo puede haber un solo usuario y funcionar bien el filtro colaborativo.

\section{Herramientas}\label{sec:herramientas}

Para el desarrollo del producto y las tecnologías previamente mencionadas, se han usado las siguientes herramientas.

\subsection{Backend}
Para el desarrollo del servidor se han usado Python con el framework web Django, ambos en sus últimas versiones, además, de librerías que permitan implementar los requisitos de la aplicación.

\subsubsection{Python}
Python es un lenguaje interpretado y multiparadigma. Esto último se debe a que soporta tanto programación orientada a objetos como programación imperativa, además de programación funcional. Actualmente está administrado por la fundación \textit{Python Software Foundation} y posee una licencia de código abierto denominada \textit{Python Software Foundation License}. Esta nos permite hacer modificaciones de su código fuente y la creación todo trabajo derivado de este, sin necesidad de que este sea también código abierto.

Se ha decidido utilizar este lenguaje de programación en el lado del servidor debido a la cantidad y facilidad de uso de herramientas que implementan las tecnologías previamente mencionadas. Es la referencia en temas de \textit{Web Scrapping} e Inteligencia Articial, como se puede apreciar en el artículo de \citeA{python}.

\subsubsection{Django}
Django es un framework de desarrollo web, también open source y que posee el patrón de diseño Modelo-Vista-Controlador. Ya no se habla de \textit{Model-View-Controller}, sino de \textit{Model-Template-View}. Sin embargo, hacen referencia a los mismos conceptos. El Modelo es la capa de accesos a los datos; la Plantilla, es la capa de presentación; la Vista posee la lógica de negocio de la aplicación y, por tanto, hace de puente entre ambos. Vemos, por tanto, que es en esencia lo mismo.

Su misión es la facilitar la creación de aplicaciones web complejas. Pone el acento en la conectividad, reutilización y la extensión de todos y cada uno de sus componentes. Tiene, además, como principal característica desde el punto de vista del desarrollo, el que todo esté hecho en Python.

Se ha decidido utilizar este framework por la facilidad de uso, y la potencia que posee sin configuración previa, es decir, posee integrado desde el primer momento aspectos avanzados de seguridad (evita inyecciones SQL, validaciones de los formularios, autentificación en cada página, entre otros), gestión en cada momento las sesiones, así como la transparencia en multitud de aspectos: la base de datos utilizada, la localización de los archivos estáticos, el despliegue…

\subsubsection{Librerías}
Los módulos son gestionados por Python a través del CLI \textit{pip}. Estos se añaden automáticamente al software a través de este. Las libreías usadas son las siguientes:

\begin{itemize}
    \item \textbf{django-rest-framework}: es la herramienta que permite ofrecer una API usando Django. Esta convierte las vistas de Django en un endpoint para publicitar contenido a través de los verbos HTTP.
    \item \textbf{django-rest-auth}: ofrece sobre la API implementada una capa de autenticación usando el modelo de usuarios de Django usando diferentes técnicas, como extracción completa de la información de usuarios o registro por redes sociales.
    \item \textbf{django-cron}: crea operaciones que se ejecutan sobre el CLI propio de Django para poder ser ejecutadas de manera recurrente. Estos métodos interaccionarán con todos los módulos de Django.
    \item \textbf{gunicorn}: es un servidor HTTP que soporta WSGI. Este ofrece a través de peticiones HTTP las aplicaciones de Django. 
    \item \textbf{beautifulsoup}: es una herramienta que permite tratar con archivos HTML para extraer información.
    \item \textbf{feedparser}: permite descargar y parsear todo tipo de Feed RSS. Además, es compatible con muchísimos formatos: desde RSS 0.9 hasta RSS 2.0; desde Atom 0.3, hasta Atom 1.0.
    \item \textbf{newspaper3k}: es una herramienta de obtención de noticias. Analiza una web y calcula la localización del contenido principal. Además, soporta la mayoría de lenguajes principales y posee herramientas de Procesamiento de Lenguaje Natural en algunos idiomas.
    \item \textbf{python-dateutil}: facilita el tratamiento con fechas en Python y su conversión a diferentes formatos.
\end{itemize}

Para elegir cada una, se ha realizado un pequeño estudio entre sus posibles alternativas. Dado que son módulos enormemente usados, son la referencia en su tema, y rara vez hay alguna alternativa de la misma embergadura que haga dudar al desarrollador.

\subsection{Frontend}
Para el desarrollo de la interfaz de usuario se ha escogido JavaScript y el framework de desarrollo VueJS.

\subsubsection{JavaScript}
Desde el inicio del desarollo web hasta hoy en día solo han exisistido tres lenguajes en el mundo del frontend: HTML, CSS y JS. El primero para la estructuración de la información, el segundo para el diseño y el tercero para la interacción del usuario. Es sobre este tercero donde mayores avances ha habido.

Ni que decir tiene que JavaScript se ha convertido en el lenguaje de referencia en el desarrollo web si se quiere una interfaz desacoplada, asíncrona y elegante, como revela la encuesta mundial de \citeA{state_js}. Son estos los mótivos por los que se ha escogido.

De hecho, si se quiere realizar una \textit{Single Page Application} (SPA en adelante), no hay otra opción plausible. Esta permite tener un simple fichero HTML e ir interactuando con él a través de JavaScript exclusivamente. Esto da al usuario una visión limpia y rápida de la aplicación en cuestión.

Se ha utilizado este sobre otros lenguajes de mayor alto nivel basados en JavaScript como TypeScript o Flow por su sencillez y recomendaciones de las herramientas utilizadas.

\subsubsection{VueJS}

Una vez decidido que se pretende realizar un frontend desacoplado del lado del servidor, solo queda la elección del framework a utilizar. Realmente, esta decisión en nuestros días se reduce a tres posibles alternativas: React, Angular y VueJS. Esa afirmación se basa en los datos y dirección de la comunidad, como se puede observar en el ya citado \citeA{state_js}.

Si se observa, aparecen, junto a estos tres, dos framework en cuanto a utilización actual: VanillaJS y AngularJS. Sin embargo, estos dos van quedando en desuso en cuanto a framework frontend de una aplicación web como tal. VanillaJS utiliza JavaScript en su estado puro, por lo que, realmente, no se le puede considerar un framework; consiste en una librería se usa cuando se quiere tener en cuenta velocidad y espacio. AngularJS, por su parte, es el predecesor de Angular (también conocido como Angular 2). Cambia por completo su funcionamiento, siendo esta primera versión una librería y la segunda y posteriores, un framework. Google, autor del mismo, ha dejado de actualizar AngularJS, por lo que no ha de tenerse en cuenta para desarrollos actuales.

Visto lo anterior, comparemos los tres frameworks mencionados para ilustrar el motivo de elegir VueJS sobre los demás, perteneciente al estudio realizado por \citeA{frameworks_js}.

\begin{itemize}
    \item \textbf{React}: es la librería de Facebook centrada en la creación de vistas. Su gran virtud son los patrones de eventos, ya que estos permiten actualizar en tiempo real las vistas con los datos. Realmente no es un framework, por lo que es posible utilizarlo anexo a Angular o VueJS. Si embargo, debido a su potencial, puede ser usado exclusivamente para realizar la capa de presentación.
    \item \textbf{Angular}: es el framework de Google diseñado sobre su primera versión y hecho para ser totalmente independiente y funcional. Usando HTML, CSS y TypeScript consigue crear vistas, componentes independientes y rutas, todas relacionados entre sí. Trabaja sobre el concepto \textit{Single Page Application} (SPA) utilizado para ello Webpack, esto es, una vez desarrollada la aplicación, se compila en un fichero HTML (\textit{index.html}), un fichero CSS y varios ficheros JS.
    \item \textbf{VueJS}: es el framework creado por la comunidad que une las ventajas de los desarrollos anteriores, a destacar: el ciclo de vida de los componentes propio de React y las directivas de Angular. La curva de aprendizaje es más reducida que Angular y el proyecto poseerá menos ficheros que este, ya que aglutina HTML, CSS y JS en un fichero \textit{.vue}. Además, es totalmente versatil: se puede utilizar como librería para un desarollo pequeño hasta como framework en un aplicación SPA o SSR completa.
\end{itemize}

Son estos motivos por los que se ha optado por VueJS, aunque sinceramente, debido a su similitud con Angular, se podría haber optado por cualquiera. Además, el ecosistema que ofrece Vue, como se verá en el siguiente punto, es único.

\subsubsection{Librerías}

Los módulos usados están relacionados con el ecosistema que ofrece Vue así como por la comunidad de JavaScript. Estos se gestionan a través de \textit{npm}.

\begin{itemize}
    \item \textbf{vue-router}: gestiona la navegabilidad entre las vistas, compartiendo datos entre sí y haciendolas accesibles desde la URL del navegador.
    \item \textbf{vuex}: administra los estados de determinados objetos a lo largo de toda la aplicación. Ofrece de manera general un almacén de estados que pueden ser mutados y consultados desde cualquier módulo.
    \item \textbf{vuetify}: implementa un ecosistema basado en Material Design, el diseño propuesto por Google, usado componentes Vue.
    \item \textbf{axios}: gestiona las peticiones HTTP en JavaScript ofreciendo una interfaz completa y simple.
    \item \textbf{moment}: convierte cualquier campo de tipo fecha al formato deseado por el usuario.
\end{itemize}

Todas estas libreías son usadas por la gran mayoría de la comunidad y no poseen alternativas claras y robustas: los desarrolladores prefieres mejorarlas a enfrentarse a ser competencia directa.

\subsection{Servidor}

En este apartado se mencionarán las tecnologías usadas en la parte del servidor, de manera conjunta a las de Cliente y Servidor previamente mencionadas.

\subsubsection{PostgreSQL}

Es imprescindible usar en este tipo de proyectos una base de datos consistente. Además, dada la naturaleza de la aplicación en cuestión, esta ha de ser Relacional. De todas las posibilidades que ofrece este gran mundo, Django recomienda usar esta debido a su naturaleza y fuerte interconexión.

Postgres es un sistema gestor de base de batos de tipo relacional, como se ha dicho. Posee una gran escalabilidad, al usar una infraestructura propia de paralelización: multiprocesos en vez de multihilos. Esto implica que desacopla los posibles errores propios de la gestión de hilos, como son las condiciones de carrera.

\subsubsection{Elasticsearch}

Para implementar la Búsqueda y Recuperación de Información, descrito en el punto \ref{sec:tecnologias}, referente a las tecnologías, es necesario usar un Índice.

Elasticsearch es un servidor de búsqueda, distribuido y accesible a través de una interfaz de tipo RESTful. Está desarrollado en Java, basándose en Lucene y es de código abierto. Propone una sintáxis basada en JSON para las búsquedas.

Se ha decidido usar sobre Índice como Algolia o Solr por dos motivos principales. El primero se debe a su coste, nulo en proyectos con implantación propia de dicha herramienta. El segundo es que se está poniendo a la cabeza en el desarrollo de índices generales en la comunidad.

\subsubsection{Nginx}

Una vez albergado y desarrollada la aplicación, es necesario hacerla accesible a través de Internet de manera segura y consistente. Para ello nos hace falta un servidor web o proxy inverso. Las dos grandes alternativas en este punto son Nginx y Apache.

Nginx es un tipo de servidor web ligero y con alto rendimiento. Es open source y permite, además, integración de servidor SMTP, así como soporte de HTTP, HTTP2, IPv6 y demás protocolos.

Se ha decidido usar este por el gran rendimiento que ofrece sobre el Servidor HTTP Apache. Por otra parte, ofrece una gran comunicación y facilidad en la configuración en aplicaciones Full Stack.

\subsubsection{Docker}

Para hacer toda esta arquitectura independiente a cualquier sistema y con la configuración similar a entornos de desarollo y producción es necesaria la virtualización de los servicios. Esto ha marcado un nuevo hito en el mundo del software: la abstracción entre sistemas operativos y la concentración de la configuración en un único fichero. Esto es Docker.

Docker automatiza el despliegue de cualquier software en contenedores. Estos poseerán toda la configuración que necesita el desarrollo para funcionar. Es la evolución de la máquina virtual: se convierten en instancias Linux ligeras y aisladas.

Actualmente, no hay una alternativa clara a Docker. De hecho, tecnologías de orquestación de contenedores, tales como Swarm o Kubernetes, se basan en esta.

\subsection{Gestión}

En cuanto a la gestión del proyecto se refiere, podemos hablar de dos grandes ámbitos: desarrollo y seguimiento del proyecto.

\subsubsection{Desarrollo}

Para el desarrollo del proyecto han sido necesarias las herramientas que se mencionarán adelante. La preferencia general de dicha elección se basa en dos premisas. La primera es la orientación de la comunidad del desarrollo y la segunda es la disponibilidad real de dichas herramientas. Además de utilizar tecnologías gratuitas, la universidad nos ofrece plataformas y programas en periodo de prueba.

En primer lugar son necesarios los programas para el desarrollo de las diferentes partes de la aplicación. Dado que cada módulo se desarrolla en un lenguaje concreto, se ha apostado por usar IDEs personalizados. Así, y debido a su potencial, se han usado todos aquellos necesarios creados por JetBrains. En el caso de no existir, se ha decidido acudir a Visual Studio Code.

Se ha usado también una herramienta para la realización de diagramas llamada Astah. Esto permite analizar lo que se necesita antes de implementarlo. Gracias a esta se harán todo tipo de diagramas: desde estados hasta clases. Se ha usado esta gracias a su gratuidad a estudiantes.

Para mantener una trazabilidad y respaldo del código, se han usado tecnologías de control de versiones. En concreto, la elegida ha sido Git, debido a su posición en la comunidad del desarrollo y a su funcionalidad y robustez. La plataforma remota donde se alojará dicho código sobre Git es GitHub. Ha sido elegida por ser la principal usada por la comunidad.

En cuanto a la comunicación y despliegue por el servidor, se ha elegido como VPS: Digital Ocean. Posee un enorme potencial y un periodo de prueba más que suficiente para la distribución de la aplicación. Además, para la comunicación con este, se ha usado WinSCP y PuTTY, como herramientas SSH principalmente usadas en Windows.

\subsubsection{Seguimiento}

Una vez llegado a las decisiones convenientes para desarrollar el producto, es necesario seguir una serie de pautas, marcarse unos hitos y tener la posibilidad de comprobar el avance del proyecto. Así, han de usarse una serie de herramientas para medir la evolución y seguir con acierto.

Se han usado grosso modo dos: Toggl para la gestión del tiempo y Waffle para la gestión de las tareas. La primera permite comprobar las horas que lleva cada tarea. Esta permite etiquetar cada una, por lo que se podrá comprobar el tiempo dedicado a cada tipo de tarea (gestión, documentación, desarrollo, despliegue o investigación), así como el total dedicado al proyecto. Con la segunda se consigue agrupar las tareas en hitos y asociarlos a funcionalidades dentro del código. Además, ambas están relacionadas, por lo que simplifica dicha gestión, siendo esta la principal motivación para su uso.

Waffle es la implementación concreta del tablero convas mencionado en el punto \ref{sec:metodologia_lean}. Si se aprecia la imagen \ref{fig:scrum_github}, vemos que es GitHub la plataforma mencionada para el tablero. Lo ocurrido se puede apreciar en el artículo de \citeA{waffle}. Esta herramienta deja de mantenerse por falta de recursos y recomienda su migración a GitHub.

\section{Estructura del proyecto}\label{sec:estructra_proyecto}

Como se ha ido dicendo a lo largo del documento, el proyecto utiliza Git. Así, se ha estructurado el trabajo en ramas según la metodología Git Flow. Este estructura el cualquier proyecto software con cinco ramas principales:

\begin{itemize}
    \item \textbf{master}: el código en esta rama es la versión estable y funcional que se encuentra funcionando en producción.
    \item \textbf{hostfix}: cualquier fallo detectado en producción se arregla aquí y se lleva a \textit{master} uuna vez solucionado: no antes.
    \item \textbf{release}: sobre los cambios agrupados del desarrollo, se prueba integralmente antes de ser subidos a \textit{master}.
    \item \textbf{development}: agrupa los cambios en las diferentes características que se van trabajando.
    \item \textbf{feature}: cada una de las funcionalidades a añadir en el sistema.
\end{itemize}

Esto se puede ver gráficamente en la siguiente imagen.

\figura{0.75}{img/estructura/git_flow}{\textit{Git Flow}}{fig:git_flow}{}

Teniendo esto en cuenta, se mostrará cómo se ha estructura cada repositorio de manera general en GitHub, para luego descender, de manera individual, a cada uno.

\subsection{GitHub}

De manera general, se poseen estos respositorios estos proyectos en la plataforma de código abierto GitHub.

\figura{0.75}{img/estructura/github_repositories}{Respositorios en GitHub}{fig:github_repositories}{}

\subsection{Backend}

\subsection{Frontend}

\section{Detalles de implementación}\label{sec:detalles_implementacion}