% !TEX root = ../../proyect.tex

\chapter{Implementación}\label{implementacion}
\section{Tecnologías}\label{sec:tecnologias}

A continuación se definirán las tecnologías usadas para llevar a cabo el Producto Mínimo Viable.

\subsubsection{\textit{Web Scrapping}}
El aspecto fundamental del producto es la recolección de noticias de periódicos. Esta, como se ha ido diciendo a lo largo del documento, se realiza a través de RSS. Sin esta característica, no tendría utilidad la aplicación. Sin embargo, esto solo se queda corto. Normalmente, cada medio solo publica un titular y una pequeña descripción vía RSS. Esto les sirve de gancho al usuario para que pase del lector de sindicación de contenido a la web de su medio.

Como se ha dicho, otro pilar fundamental para el buen funcionamiento del producto es el análisis de noticias. Por tanto, si no se posee más que un pequeño texto acompañado de cada noticia, poco podrá aportar este. Es por ello imprescindible conseguir el texto original del artículo para poder procesarlo correctamente. Dado que los medios no poseen esta característica, hemos de realizarla a través del \textit{Web Scrapping}.

El \textit{Web Scrapping} es una técnica que permite extraer información de cualquier sitio en Internet de forma automática. La información en la web se encuentra en diferentes formatos dependiendo de su misión para con el receptor. Resumidamente se encuentran dos tipos de datos: estructurados, como pueden ser ficheros XML o APIs, y no estructurados, como los ficheros HTML. Estos últimos abundan ya que, en un principio, van destinados al usuario final directamente, con la idea de ser leídos o vistos sin más.

\figura{0.75}{img/desarrollo/web_scrapping}{Muestra del \textit{Web Scrapping}}{fig:web_scrapping}{}

Por lo dicho, esta técnica posee diferentes implementaciones, dependiendo del nivel de automatización que desee. Un primer nivel sería el de extraer el fichero HTML y trabajar con él, utilizando, por ejemplo, expresiones regulares; otro sería, utilizar parsers o minería de datos, para extraer el contenido principal; y otro, el de analizar una página en concreto, para saber cómo se estructura el contenido realmente interesante.

Como última consideración, se ha informar que antes de trabajar con la información extraída del \textit{Web Scrapping}, hay que saber que esta tiene implicaciones legales. Estas, en la mayoría de los casos, se encuentran en un vacío legal. En la práctica, si los datos se usan para un uso personal, no hay problema añadido. La cosa cambia si es para un uso comercial. De hecho, actualmente, hay varios juicios por un uso fraudulento de esta técnica en Australia o Estados Unidos, como se puede ver en el artículo publicado por \citeA{web_scrapping}.

\subsubsection{Recuperación de información}
Una vez extraída la información de los medios, es imprescindible tratarla correctamente. No basta el guardarla en una base de datos sin más, sino que es necesario usar medios de albergar la información especializado para ello. Si se usa, por ello, ténicas de Búsqueda y Recuperación de Información (ISR, por sus siglar en inglés, en adelante) podemos tratar cada noticia como un documento y así extraer metadatos de cada uno.

\figura{0.75}{img/desarrollo/information_retrieval}{Muestra de Recuperación de Información}{fig:information_retrieval}{}

Esta técnica, por tanto, permite recuperar documentos en base a búsqueda de sus metadatos guardados en un índice. Esta información no suele estar estructurada y su misión es la de buscar información relevante en base a diferentes filtros. Hay diferentes técnicas, dependiendo del grado de exactitud que se requiera teniendo en cuenta la consulta. Algunas de las más importantes son las siguientes.

\begin{itemize}
    \item \textbf{Sistemas de recuperación de lógica difusa}: las búsquedas sobre la información son de tipo fuzzy, es decir, frases normales, que el sistema transforma en frases lógicas. Así, la información que se consegue, se basa en proposiciones lógicas, teniendo en cuenta si los términos buscados aparecen o no.
    \item \textbf{Técnicas de ponderación de términos}: dado que hay términos de búsqueda que tienen más relevancia que otros, es lógico que se pondere dichos términos. Así, los documentos más pertinentes, serán los que contengan todos los términos y que se repita más el que más valor tenga.
    \item \textbf{Técnicas de clustering}: es un modelo probabilístico sobre la frecuencia de los términos de búsqueda. Se clasifican los documentos por valores que, finalmente, representan los pesos.
    \item \textbf{Técnicas de retroalimentación por relevancia}: una vez hecha una búsqueda utilizando una de las técnicas anteriores, se vuelve a buscar pero con términos relacionados con los primeros documentos resultantes.
    \item \textbf{Técnicas de stemming}: usa la técnica de Stemming para la búsqueda, es decir, trunca los términos de búsqueda quitante posibles prefijos o sufijos. En fin, evita ambigüedades léxicas.
\end{itemize}

Como se puede observar, estas técnicas se pueden combinar entre sí dentro de una implementación concreta, aunque siempre el funcionamiento es el presentado en la imagen \ref{fig:information_retrieval}: se procesan los documentos, se indexan y se busca sobre el índice.

Como se ha dicho al inicio del apartado, interesa que este índice creado trate correctamente la información guardardada. Esta es, por tanto, la mayor ventaja de usar un ISR: se pueden elegir técnicas y tratamientos concretos por cada campo, así como por cada índice de documentos. En el ejemplo de guardar noticias se puede decir que extraiga las palabras claves, que de más importancia al cuerpo de la noticia, que obvie mayúsculas y tildes en el índice y para el análisis, que no tenga en cuenta artículos y preposiciones.

Esto dará una versatilidad a esta \textit{base de datos no relacional} mucho mayor, ya que estará especializada en el dominio del problema concreto. Mientras tanto, en una base de datos clásica, la información se guarda siempre de manera homogénea y genérica para cualquier dato que se quiera albergar.

\subsubsection{Sistemas de recomendación}
Una vez obtenidas las noticias y analizadas y albergadas en el sistema habrá que dar uso a los metadatos albergados. Esta, como se ha dicho a la hora de explicar el producto, será el valor diferencial a cualquier lector de noticias: la capacidad de extraer el perfil del usuario y recomendar noticias en base a sus intereses. Esto, por tanto, se consigue con un sistema de recomendación especializado.

\figura{0.5}{img/desarrollo/recommendation_system}{Muestra de sistemas de recomendación}{fig:recommendation_system}{}

Es la técnica de filtrado de información dependiendo del interés del usuario en los mismos. En definitiva, se encarga de comparar el perfil del usuario con los ítems en concreto para, finalmente, recomendarle uno que no haya visto previamente. Para realizar esta técnica, tenemos la capacidad de elección entre diversas implementaciones. Como se puede apreciar, es importante analizar tanto a usuarios como ítems a recomendar.

Analizar el perfil del usuario significa, en definitiva, extraer características de su interactuación con los ítems. El método de extracción de sus características puede ser implícito o explícito. Dentro del primero encontramos aquellos sistemas a los que el sistema solicita al usuario que valore un determinado producto, o, directamente, seleccione sus intereses. Dentro del segundo, a aquellos en los que, en base a visitas a un ítem, al tipo de interacción con el mismo, al tiempo que está viéndolo, u otras técnicas, saca información del usuario.

Ambos tienen aspectos positivos y negativos. En el primero, el usuario dice al sistema expresamente qué le gusta o disgusta, mientras el segundo, la aplicación ha de suponerlo. Esto último puede llevar a numerosos fallos de interpretación frente a la certeza que proporciona el primero. Por otro lado, el método implícito extrae dicha información sin necesidad de la intervención del usuario, cosa que el primero no puede. Esto puede hacer que en el método explícito el usuario nunca complete sus preferencias, por el poco uso de la aplicación.

Como se podrá intuir, la mejor solución depende de la naturaleza de la aplicación y el enfoque que ponga esta en el usuario objetivo. No obstante, una buena solución podrá ser un aproximación a ambas, de forma híbrida. Con esto, se poseerá las ventajas de ambas de manera sencilla.

\figura{0.75}{img/desarrollo/user_profile}{Muestra de extracción del perfil del usuario}{fig:user_profile}{}

Una vez obtenido el perfil del usuario, hemos de poder recomendarle ítems. En este apartado encontramos dos enfoques mayoritarios (y un tercera, de la unión de ambos) como se puede apreciar en la imagen \ref{fig:recommendation_system}.

La primera implementación son las recomendaciones basadas en contenido. Esta técnica consiste en analizar los ítems que tenemos en nuestros sistemas en base a una o varias características. Así, sabiendo los ítems con los que el usuario ha interactuado, podemos recomendarle otros que posean características similares. El principal problema de este sistema de recomendación es la falta de serendipia, es decir, al usuario siempre se le muestra el mismo tipo de productos. Esto se debe a es la información que se tiene y sobre eso se recomienda. Esto conllevará un hartazgo de las recomendaciones, resultando finalmente inútiles.

La segunda implementación es el filtrado colaborativo. Esta consiste en mostrar productos que han resultado atractivos para otros usuarios con gustos similares y que todavía no ha visto. Para esto, se han de calcular similitud entre los usuarios, y no sobre los ítems, como vimos en las recomendaciones en base al contenido. El principal problema del este es que funcionará bien si se poseen diferentes y numerosos tipos de usuario y si se tiene del usuario en cuestión de bastante información para tener un perfil complejo que comparar con otros usuarios. Esto es un problema, para aplicaciones que se empiezan desde cero sin usuarios.

Aquí, igual que anteriormente con el perfil del usuario, es posible combinar ambas técnicas haciendo un filtrado híbrido. Esto hará que el se contrarresten los defectos de ambas, centrándose en sus virtudes. Con respecto a la falta de serendipia, el filtro colaborativo lo contrarresta haciendo similitud entre usuarios, por que el usuario puede verse sorprendido al ver que se le recomienda algo que puede ser nuevo para él. Con respecto a un gran número de usuarios necesarios inicialmente, las recomendaciones basadas en contenido lo contrarrestan por la similitud de ítems, por lo que, por ejemplo, lo puede haber un solo usuario y funcionar bien el filtro colaborativo.

\section{Herramientas}\label{sec:herramientas}

Para el desarrollo del producto y las tecnologías previamente mencionadas, se han usado las siguientes herramientas.

\subsection{Backend}
Para el desarrollo del servidor se han usado Python con el framework web Django, ambos en sus últimas versiones, además, de librerías que permitan implementar los requisitos de la aplicación.

\subsubsection{Python}
Python es un lenguaje interpretado y multiparadigma. Esto último se debe a que soporta tanto programación orientada a objetos como programación imperativa, además de programación funcional. Actualmente está administrado por la fundación \textit{Python Software Foundation} y posee una licencia de código abierto denominada \textit{Python Software Foundation License}. Esta nos permite hacer modificaciones de su código fuente y la creación todo trabajo derivado de este, sin necesidad de que este sea también código abierto.

Se ha decidido utilizar este lenguaje de programación en el lado del servidor debido a la cantidad y facilidad de uso de herramientas que implementan las tecnologías previamente mencionadas. Es la referencia en temas de \textit{Web Scrapping} e Inteligencia Articial, como se puede apreciar en el artículo de \citeA{python}.

\subsubsection{Django}
Django es un framework de desarrollo web, también open source y que posee el patrón de diseño Modelo-Vista-Controlador. Ya no se habla de \textit{Model-View-Controller}, sino de \textit{Model-Template-View}. Sin embargo, hacen referencia a los mismos conceptos. El Modelo es la capa de accesos a los datos; la Plantilla, es la capa de presentación; la Vista posee la lógica de negocio de la aplicación y, por tanto, hace de puente entre ambos. Vemos, por tanto, que es en esencia lo mismo.

Su misión es la facilitar la creación de aplicaciones web complejas. Pone el acento en la conectividad, reutilización y la extensión de todos y cada uno de sus componentes. Tiene, además, como principal característica desde el punto de vista del desarrollo, el que todo esté hecho en Python.

Se ha decidido utilizar este framework por la facilidad de uso, y la potencia que posee sin configuración previa, es decir, posee integrado desde el primer momento aspectos avanzados de seguridad (evita inyecciones SQL, validaciones de los formularios, autentificación en cada página, entre otros), gestión en cada momento las sesiones, así como la transparencia en multitud de aspectos: la base de datos utilizada, la localización de los archivos estáticos, el despliegue…

\subsubsection{Librerías}
Los módulos son gestionados por Python a través del CLI \textit{pip}. Estos se añaden automáticamente al software a través de este. Las libreías usadas son las siguientes:

\begin{itemize}
    \item \textbf{django-rest-framework}: es la herramienta que permite ofrecer una API usando Django. Esta convierte las vistas de Django en un endpoint para publicitar contenido a través de los verbos HTTP.
    \item \textbf{django-rest-auth}: ofrece sobre la API implementada una capa de autenticación usando el modelo de usuarios de Django usando diferentes técnicas, como extracción completa de la información de usuarios o registro por redes sociales.
    \item \textbf{django-cron}: crea operaciones que se ejecutan sobre el CLI propio de Django para poder ser ejecutadas de manera recurrente. Estos métodos interaccionarán con todos los módulos de Django.
    \item \textbf{gunicorn}: es un servidor HTTP que soporta WSGI. Este ofrece a través de peticiones HTTP las aplicaciones de Django. 
    \item \textbf{beautifulsoup}: es una herramienta que permite tratar con archivos HTML para extraer información.
    \item \textbf{feedparser}: permite descargar y parsear todo tipo de Feed RSS. Además, es compatible con muchísimos formatos: desde RSS 0.9 hasta RSS 2.0; desde Atom 0.3, hasta Atom 1.0.
    \item \textbf{newspaper3k}: es una herramienta de obtención de noticias. Analiza una web y calcula la localización del contenido principal. Además, soporta la mayoría de lenguajes principales y posee herramientas de Procesamiento de Lenguaje Natural en algunos idiomas.
    \item \textbf{python-dateutil}: facilita el tratamiento con fechas en Python y su conversión a diferentes formatos.
\end{itemize}

Para elegir cada una, se ha realizado un pequeño estudio entre sus posibles alternativas. Dado que son módulos enormemente usados, son la referencia en su tema, y rara vez hay alguna alternativa de la misma embergadura que haga dudar al desarrollador.

\subsection{Frontend}
Para el desarrollo de la interfaz de usuario se ha escogido JavaScript y el framework de desarrollo VueJS.

\subsubsection{JavaScript}
Desde el inicio del desarollo web hasta hoy en día solo han exisistido tres lenguajes en el mundo del frontend: HTML, CSS y JS. El primero para la estructuración de la información, el segundo para el diseño y el tercero para la interacción del usuario. Es sobre este tercero donde mayores avances ha habido.

Ni que decir tiene que JavaScript se ha convertido en el lenguaje de referencia en el desarrollo web si se quiere una interfaz desacoplada, asíncrona y elegante, como revela la encuesta mundial de \citeA{state_js}. Son estos los mótivos por los que se ha escogido.

De hecho, si se quiere realizar una \textit{Single Page Application} (SPA en adelante), no hay otra opción plausible. Esta permite tener un simple fichero HTML e ir interactuando con él a través de JavaScript exclusivamente. Esto da al usuario una visión limpia y rápida de la aplicación en cuestión.

Se ha utilizado este sobre otros lenguajes de mayor alto nivel basados en JavaScript como TypeScript o Flow por su sencillez y recomendaciones de las herramientas utilizadas.

\subsubsection{VueJS}


\subsection{Servidor}

\subsection{Gestión}

\section{Estructura del proyecto}\label{sec:estructra_proyecto}

\subsection{Backend}

\subsection{Frontend}

\section{Detalles de implementación}\label{sec:detalles_implementacion}